{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zuyeonn/2025-2-NLP/blob/main/unsloth_fine_tune_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtw7univhR3D"
      },
      "source": [
        "# 0: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlmF7vbkgDQb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.0\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQNDmGy0ginc",
        "outputId": "5989dc7a-db62-41b1-de28-65cc1bff141f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "work_path = '/gdrive/MyDrive/Sungshin/2025-2-NLP/fine-tune-tutorial'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op-7sONlhH_q"
      },
      "source": [
        "### Specify the pretrained model as `LLama-3.1-8B-Instruct-bnb-4bit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "466f9cfee6b6496b8a25cf7989962e1c",
            "4492b19cb625442b84c705355df964ae",
            "ba4d1161388e48a1a45ebe3292d60773",
            "d07df67180db4799966d7434e1a06ccf",
            "834f13a0a8c947d5ac0d2c94a216f7f1",
            "9e26e1c5dda5429a866b10d4a629509f",
            "04c96ec689b24618931c5f010216fd14",
            "106411e2a4e64422abb767cd4aca7fc4",
            "9f854834fdbc4980919c419def889dd5",
            "0157fa6c577e4c789884b179e8418909",
            "6e18f9099da242ff83c9dd79307a0db1",
            "a3f9ce6c960e4d188e2480b22b28df3d",
            "6b29aa8b89ca43c08fda6a67260ad705",
            "325c0ef9377a4655acbf3efbea9844b8",
            "499e6e923fe34ba2b166fc8c0fcb0bd0",
            "e0fd1b95d65649ce8ab020f30c395695",
            "5d23da164ea947bdbfdfa63b81ae908f",
            "88a5e07df86846c0bd8644097f4c6f4d",
            "d0e22d1ee34d4152a2efc02f01529984",
            "1df118f0a75d43aa8a537d020e7ca454",
            "6a8c37ffbcec460f9720eed15aac6fad",
            "f0240546c9cf4e81b27fd705fbd47b64",
            "37f2d71c05d64ff1981a29c539d6cd4b",
            "e9a9300ca7794783ba19ec0936674252",
            "0d078afd154f4fc5b65d5acbe44d1a7c",
            "9ee5ce007e6049aaa9d042c83809e582",
            "e6fb590a5cf04d5ebe277ae8c0387258",
            "0f1cdaf42ade44fcbcda5007abde6c81",
            "1013e160d35c4568a9db836d97eacdb0",
            "717fb28b7f4b41698f464256e6a71f01",
            "c5dbddc7abd7497ba9e7ba34f7178b78",
            "39ca97824bb143d28561fd245c8ab820",
            "50c88a84708b4180b3d724b7662a2f38",
            "1c031d1907a94f9bbb7b5fc3b9fc2b65",
            "a5c6ae7dbaf34ae6a3db47603e83e29e",
            "a590576c2cbd455bbf433e98665385b5",
            "e930b74fdfdd442881c4aea37beb4a8f",
            "d2deb38a47d6497c92414e0c17d81925",
            "b91e71b9cfe240bbb6771a483ed61bef",
            "be76279baf4644d1932a2fd669dedd57",
            "d62878acf4c246e28962e04b9b4c6903",
            "e4092a717e1f438880a3933797c87d8e",
            "73574d3e815c4d05b6f44b8aadc8db97",
            "6686bb2d95ef4780a86aa49028790476",
            "775e7bbf80244f91aa8b05b5dae6f6bf",
            "bf556779be1c46e197c31b9b9b886e8c",
            "faeb61dd9682489db3d6287b43c7c625",
            "fbc4dff8a0224205bd19942e5edeea0a",
            "34748e3c0c574685ba70015363d3c1c6",
            "640f4f6b1ed848a99de49c22d1c07292",
            "adfb7b4b4f504aafaad093cdd550052d",
            "a61e0c0159f842e392369592448a00e6",
            "6a1f4ea0c4634da088cf4fc8dad18fd0",
            "5059c03252fc486182febbc4de1a3001",
            "1a325787c72e4f8cade20b834e7e83eb"
          ]
        },
        "id": "R7ADAjMog62D",
        "outputId": "495f95a0-dc51-497b-d882-b5389af59d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.9.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Unsloth: Xformers was not installed correctly.\n",
            "Please install xformers separately first.\n",
            "Then confirm if it's correctly installed by running:\n",
            "python -m xformers.info\n",
            "\n",
            "Longer error message:\n",
            "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.9.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "466f9cfee6b6496b8a25cf7989962e1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3f9ce6c960e4d188e2480b22b28df3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37f2d71c05d64ff1981a29c539d6cd4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c031d1907a94f9bbb7b5fc3b9fc2b65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "775e7bbf80244f91aa8b05b5dae6f6bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # context Í∏∏Ïù¥\n",
        "dtype = None           # GPUÏóê ÎßûÍ≤å ÏûêÎèôÏÑ§Ï†ï\n",
        "load_in_4bit = True    # 4bit ÏñëÏûêÌôî\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\",  # ÌïÑÏöîÌïòÎ©¥ HF ÌÜ†ÌÅ∞\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5RNCLnVhONT"
      },
      "source": [
        "LoRA Setup: Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPeGLgcwhJDA",
        "outputId": "550966a1-dc86-44d9-b604-cd2198a31cd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.11.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLARpCwFhcOz"
      },
      "source": [
        "# 1: Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a59163a9cc0040da8bc2d249ca0fb087",
            "2e655b53f28e4a62b64c79a59aea434f",
            "71c6119af9574374abfb1418e8208c14",
            "129b504d022c499ba288d54471cfcb78",
            "a7e99a9e06694f8097a23b1127347ff5",
            "063be4d819534337872348832c5cf2ee",
            "f072fb8986bb45a0b0f4379abc408a5f",
            "9c96e3d5413048429a0757af677f5b9f",
            "764f73af44b64ae2aab6d6eab61d55e9",
            "e2e11b749b354c1b8de046a003c4c71c",
            "ae6f81163f284fd685166e9936b4d326"
          ]
        },
        "id": "DZabjZ9lhYiz",
        "outputId": "ccac1eda-4ed0-4730-a864-149b2b288116"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a59163a9cc0040da8bc2d249ca0fb087",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from the JSON file\n",
        "raw_dataset = load_dataset(\"json\", data_files=f\"{work_path}/order_analysis-dataset.json\", split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLAx4O3iiIRJ"
      },
      "source": [
        "##  Q1: Split the dataset into training and test sets using an 80:20 ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZj-fpBFhjOF",
        "outputId": "0b14fd09-2f24-4354-f009-f7b7f0cd4014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['instruction', 'input', 'output']\n",
            "{'instruction': 'ÎÑàÎäî ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú Ï£ºÎ¨∏ Î¨∏Ïû•ÏùÑ Î∂ÑÏÑùÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏Ïù¥Îã§. Ï£ºÎ¨∏ÏúºÎ°úÎ∂ÄÌÑ∞ Ïù¥Î•º Íµ¨ÏÑ±ÌïòÎäî ÏùåÏãùÎ™Ö, ÏòµÏÖòÎ™Ö, ÏàòÎüâÏùÑ Ï∞®Î°ÄÎåÄÎ°ú Ï∂îÏ∂úÌï¥Ïïº ÌïúÎã§.', 'input': 'Ï£ºÎ¨∏ Î¨∏Ïû•: ÌïúÏö∞Ïä§ÌéòÏÖú 1Ïù∏Î∂Ñ Ï£ºÎ¨∏Ìï†Í≤åÏöî. Í∑∏Î¶¨Í≥† ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú ÌïòÎÇò Ï£ºÏÑ∏Ïöî.', 'output': '- Î∂ÑÏÑù Í≤∞Í≥º 0: ÏùåÏãùÎ™Ö:ÌïúÏö∞Ïä§ÌéòÏÖú,ÏàòÎüâ:1Ïù∏Î∂Ñ \\n- Î∂ÑÏÑù Í≤∞Í≥º 1: ÏùåÏãùÎ™Ö:ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú, ÏàòÎüâ:ÌïòÎÇò '}\n"
          ]
        }
      ],
      "source": [
        "# Q1: 80:20 ÎπÑÏú®Î°ú train/test Î∂ÑÌï†\n",
        "dataset = raw_dataset.train_test_split(test_size=0.2, seed=3407)\n",
        "\n",
        "# ÏõêÎ≥∏(=train split) ÌôïÏù∏\n",
        "print(dataset[\"train\"].column_names)\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGOelokTiLfC"
      },
      "source": [
        "### Q2: Transform dataset's format int ChatML\n",
        "\n",
        "instruction -> system\n",
        "\n",
        "input -> user\n",
        "\n",
        "output -> assistant\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": input},\n",
        "    {\"role\": \"assistant\", \"content\": output}\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWCCnCjNhleV"
      },
      "outputs": [],
      "source": [
        "def convert_to_chatml_format(examples):\n",
        "    systems = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    texts = []\n",
        "    for s, inp, out in zip(systems, inputs, outputs):\n",
        "        messages = [\n",
        "            {\"role\": \"system\",    \"content\": s},\n",
        "            {\"role\": \"user\",      \"content\": inp},\n",
        "            {\"role\": \"assistant\", \"content\": out},\n",
        "        ]\n",
        "        texts.append(messages)\n",
        "\n",
        "    return {\"conversations\": texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "d28aabeefe2b4c16810ca4f4578db735",
            "1d2b745e40274ef29368acc5597bde6d",
            "4bfd7852e0c549688f74c04dbcff2096",
            "261763d460104070a4fe941d7eb6c9f6",
            "24041532a8dc4f4c9cc8d2a3f0fd2a78",
            "66891abf3b8a4f2cb85014d9a42a9416",
            "5b5d1658cce54729882dac87b30a4b16",
            "6119d8bcc7ef436da6af1fa386aa0548",
            "39fac5ab2a2443de8f1200bdf48b271d",
            "5227b93a29d94f5cbf457a84906a9dfd",
            "051e720b87114a9a8d12486d0c18bacc",
            "d83fb2a92d0b4d4e83dcf64005721429",
            "29c0c3ffeccd4ff19a4969b78e5c018b",
            "c4c720f4fa174a889766ff2d9def2a2b",
            "e9424e6ed6ce4c1b81e51f946ec3c08d",
            "dadbaecebbab4f54aeb8f891d97c13e5",
            "d48bc1e7d22f48cbbc6462cb35b3e6dd",
            "c0f0c3cb1c4b472aad177462b4eb0f50",
            "54035aa41aa84347b1e7c6411ee2bf95",
            "32c663c0d11943fc9ad003a7b74726e7",
            "48c591abb6b84fddbea578e52e695471",
            "4890779707c64d38b5b129f26907d11f"
          ]
        },
        "id": "xdN8PiLziOjs",
        "outputId": "29b406a3-1b42-4b27-9516-d77a63239be8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d28aabeefe2b4c16810ca4f4578db735",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d83fb2a92d0b4d4e83dcf64005721429",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'instruction': 'ÎÑàÎäî ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú Ï£ºÎ¨∏ Î¨∏Ïû•ÏùÑ Î∂ÑÏÑùÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏Ïù¥Îã§. Ï£ºÎ¨∏ÏúºÎ°úÎ∂ÄÌÑ∞ Ïù¥Î•º Íµ¨ÏÑ±ÌïòÎäî ÏùåÏãùÎ™Ö, ÏòµÏÖòÎ™Ö, ÏàòÎüâÏùÑ Ï∞®Î°ÄÎåÄÎ°ú Ï∂îÏ∂úÌï¥Ïïº ÌïúÎã§.',\n",
              " 'input': 'Ï£ºÎ¨∏ Î¨∏Ïû•: ÌïúÏö∞Ïä§ÌéòÏÖú 1Ïù∏Î∂Ñ Ï£ºÎ¨∏Ìï†Í≤åÏöî. Í∑∏Î¶¨Í≥† ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú ÌïòÎÇò Ï£ºÏÑ∏Ïöî.',\n",
              " 'output': '- Î∂ÑÏÑù Í≤∞Í≥º 0: ÏùåÏãùÎ™Ö:ÌïúÏö∞Ïä§ÌéòÏÖú,ÏàòÎüâ:1Ïù∏Î∂Ñ \\n- Î∂ÑÏÑù Í≤∞Í≥º 1: ÏùåÏãùÎ™Ö:ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú, ÏàòÎüâ:ÌïòÎÇò ',\n",
              " 'conversations': [{'content': 'ÎÑàÎäî ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú Ï£ºÎ¨∏ Î¨∏Ïû•ÏùÑ Î∂ÑÏÑùÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏Ïù¥Îã§. Ï£ºÎ¨∏ÏúºÎ°úÎ∂ÄÌÑ∞ Ïù¥Î•º Íµ¨ÏÑ±ÌïòÎäî ÏùåÏãùÎ™Ö, ÏòµÏÖòÎ™Ö, ÏàòÎüâÏùÑ Ï∞®Î°ÄÎåÄÎ°ú Ï∂îÏ∂úÌï¥Ïïº ÌïúÎã§.',\n",
              "   'role': 'system'},\n",
              "  {'content': 'Ï£ºÎ¨∏ Î¨∏Ïû•: ÌïúÏö∞Ïä§ÌéòÏÖú 1Ïù∏Î∂Ñ Ï£ºÎ¨∏Ìï†Í≤åÏöî. Í∑∏Î¶¨Í≥† ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú ÌïòÎÇò Ï£ºÏÑ∏Ïöî.', 'role': 'user'},\n",
              "  {'content': '- Î∂ÑÏÑù Í≤∞Í≥º 0: ÏùåÏãùÎ™Ö:ÌïúÏö∞Ïä§ÌéòÏÖú,ÏàòÎüâ:1Ïù∏Î∂Ñ \\n- Î∂ÑÏÑù Í≤∞Í≥º 1: ÏùåÏãùÎ™Ö:ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú, ÏàòÎüâ:ÌïòÎÇò ',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train / test Îëò Îã§Ïóê Ï†ÅÏö©\n",
        "dataset = dataset.map(convert_to_chatml_format, batched=True)\n",
        "\n",
        "# Î≥ÄÌôò Í≤∞Í≥º ÏòàÏãú ÌôïÏù∏\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkFAa88Kibk5"
      },
      "source": [
        "### Q3: Apply `apply_chat_template`\n",
        "\n",
        "Roles:\n",
        "- Special tokens representing system, user, and assistant were added.\n",
        "- These special tokens are implemented differently across various models, so a unified interface, apply_chat_template, is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "c390edfff602412e85a0fb31c60e3c5e",
            "391001325ce74daab021561133a36932",
            "1068ff99ebed4b6fae4f8bd9fcc61fc3",
            "9b84c8fad63b4de1a8a25a60d69ddc61",
            "daaac379e38e4a06ba899574712f89c0",
            "04ebf069f7944058a580e79d34c582c7",
            "31c08e75910b49a48813be1ccd604941",
            "c8b88f0793f74660acc76f6969571068",
            "db805d1f10cd4ffe88a41a22c042a440",
            "58b79538d9d84bb9a1b07f39bc62000f",
            "8986c556ffed4c1cbfb120ac7cc99520",
            "c2e6814b4f074a55be256229fe1e5beb",
            "cf4ad006207b4e788721ffc76002d07c",
            "7cba215ce523464eabfeb5d8d7cc6c0d",
            "bb7aeea27c764aa08a9efa0f7bc0320d",
            "0844c4f2fe4047d39029a8c03f52dc1b",
            "31641d1a1bfa49659d5c031dd97a3421",
            "609cda2993fb48108be1ef721aa0e113",
            "e38958c5c00b445eb492fe9dcec0dfcc",
            "5a5a2b3225c54045be27624f96295a9f",
            "30240c190b524864a944eea5cec7ffbf",
            "04109f65058b4877b02fb860ac2cfac9"
          ]
        },
        "id": "P7W91IuGiVQL",
        "outputId": "1d7d4a12-af72-440c-c1b2-7d667d061c61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c390edfff602412e85a0fb31c60e3c5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2e6814b4f074a55be256229fe1e5beb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import apply_chat_template\n",
        "\n",
        "# conversations ‚Üí text Î°ú Î≥ÄÌôò (Î™®Îç∏Ïù¥ Ïã§Ï†úÎ°ú Î≥º Î¨∏ÏûêÏó¥)\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPCVzHTzifaR",
        "outputId": "30603b0c-0306-40b3-eb56-973f6d6e8438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': 'ÎÑàÎäî ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú Ï£ºÎ¨∏ Î¨∏Ïû•ÏùÑ Î∂ÑÏÑùÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏Ïù¥Îã§. Ï£ºÎ¨∏ÏúºÎ°úÎ∂ÄÌÑ∞ Ïù¥Î•º Íµ¨ÏÑ±ÌïòÎäî ÏùåÏãùÎ™Ö, ÏòµÏÖòÎ™Ö, ÏàòÎüâÏùÑ Ï∞®Î°ÄÎåÄÎ°ú Ï∂îÏ∂úÌï¥Ïïº ÌïúÎã§.',\n",
              " 'input': 'Ï£ºÎ¨∏ Î¨∏Ïû•: ÌïúÏö∞Ïä§ÌéòÏÖú 1Ïù∏Î∂Ñ Ï£ºÎ¨∏Ìï†Í≤åÏöî. Í∑∏Î¶¨Í≥† ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú ÌïòÎÇò Ï£ºÏÑ∏Ïöî.',\n",
              " 'output': '- Î∂ÑÏÑù Í≤∞Í≥º 0: ÏùåÏãùÎ™Ö:ÌïúÏö∞Ïä§ÌéòÏÖú,ÏàòÎüâ:1Ïù∏Î∂Ñ \\n- Î∂ÑÏÑù Í≤∞Í≥º 1: ÏùåÏãùÎ™Ö:ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú, ÏàòÎüâ:ÌïòÎÇò ',\n",
              " 'conversations': [{'content': 'ÎÑàÎäî ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú Ï£ºÎ¨∏ Î¨∏Ïû•ÏùÑ Î∂ÑÏÑùÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏Ïù¥Îã§. Ï£ºÎ¨∏ÏúºÎ°úÎ∂ÄÌÑ∞ Ïù¥Î•º Íµ¨ÏÑ±ÌïòÎäî ÏùåÏãùÎ™Ö, ÏòµÏÖòÎ™Ö, ÏàòÎüâÏùÑ Ï∞®Î°ÄÎåÄÎ°ú Ï∂îÏ∂úÌï¥Ïïº ÌïúÎã§.',\n",
              "   'role': 'system'},\n",
              "  {'content': 'Ï£ºÎ¨∏ Î¨∏Ïû•: ÌïúÏö∞Ïä§ÌéòÏÖú 1Ïù∏Î∂Ñ Ï£ºÎ¨∏Ìï†Í≤åÏöî. Í∑∏Î¶¨Í≥† ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú ÌïòÎÇò Ï£ºÏÑ∏Ïöî.', 'role': 'user'},\n",
              "  {'content': '- Î∂ÑÏÑù Í≤∞Í≥º 0: ÏùåÏãùÎ™Ö:ÌïúÏö∞Ïä§ÌéòÏÖú,ÏàòÎüâ:1Ïù∏Î∂Ñ \\n- Î∂ÑÏÑù Í≤∞Í≥º 1: ÏùåÏãùÎ™Ö:ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú, ÏàòÎüâ:ÌïòÎÇò ',\n",
              "   'role': 'assistant'}],\n",
              " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nÎÑàÎäî ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú Ï£ºÎ¨∏ Î¨∏Ïû•ÏùÑ Î∂ÑÏÑùÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏Ïù¥Îã§. Ï£ºÎ¨∏ÏúºÎ°úÎ∂ÄÌÑ∞ Ïù¥Î•º Íµ¨ÏÑ±ÌïòÎäî ÏùåÏãùÎ™Ö, ÏòµÏÖòÎ™Ö, ÏàòÎüâÏùÑ Ï∞®Î°ÄÎåÄÎ°ú Ï∂îÏ∂úÌï¥Ïïº ÌïúÎã§.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nÏ£ºÎ¨∏ Î¨∏Ïû•: ÌïúÏö∞Ïä§ÌéòÏÖú 1Ïù∏Î∂Ñ Ï£ºÎ¨∏Ìï†Í≤åÏöî. Í∑∏Î¶¨Í≥† ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú ÌïòÎÇò Ï£ºÏÑ∏Ïöî.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n- Î∂ÑÏÑù Í≤∞Í≥º 0: ÏùåÏãùÎ™Ö:ÌïúÏö∞Ïä§ÌéòÏÖú,ÏàòÎüâ:1Ïù∏Î∂Ñ \\n- Î∂ÑÏÑù Í≤∞Í≥º 1: ÏùåÏãùÎ™Ö:ÏàòÏ†úÏ∞® Î©îÎ¶¨Í≥®Îìú, ÏàòÎüâ:ÌïòÎÇò <|eot_id|>'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GifKRSgmijkb"
      },
      "source": [
        "#2: Training the Model\n",
        "The model is trained using Hugging Face TRL's `SFTTrainer`.\n",
        "\n",
        "For more detailed information, please refer to the TRL SFT docs.\n",
        "\n",
        "Due to time constraints, we will only run 60 steps.\n",
        "\n",
        "When performing actual fine-tuning:\n",
        "- Set num_train_epochs=1 or more.\n",
        "- Set max_steps=None."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6b82ec5e046f4632bdadb73695b1ed29",
            "7505edf514754079b52804de51a4b94a",
            "b1e8cf9ae413472bad693f64649c580d",
            "1b3b0ebb786841028ffdc5e183c9ea79",
            "e18685ae5e104fdba7156795f00f5d24",
            "cd809c63be554bae8200c6ade0dc5a9f",
            "8aeeb7a72a0348a6bfbe651d37aefa0e",
            "7a98831e1ebc44df8d22c13f24b2b9c1",
            "9c9e7a7924ef46f0ad9b854792dd2bfa",
            "b43bc512790645f69de118cacde907ad",
            "efd0bea63e9b4a9c8b6b4946660b7d6b"
          ]
        },
        "id": "7Vjmjr05ihmR",
        "outputId": "e24e5f51-6521-43f6-f5d0-b7e7cf8597f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b82ec5e046f4632bdadb73695b1ed29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset['train'],\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        num_train_epochs = 1, # For longer training runs!\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU1MuYqxipf4",
        "outputId": "036a5e02-654c-49e9-b6da-5caf635e0336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "6.883 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# ÌïôÏäµ Ï†Ñ GPU Î©îÎ™®Î¶¨ ÏÉÅÌô© ÌôïÏù∏\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "BaZnBiMeiut2",
        "outputId": "7fa01278-2a89-48f2-bd1a-b29e22d9dd7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,400 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 05:42, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.984100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.767800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.579100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.546600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIecEB0IiwsB",
        "outputId": "800792e8-04c1-41cf-fc21-ca67d6d2cde5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "381.6844 seconds used for training.\n",
            "6.36 minutes used for training.\n",
            "Peak reserved memory = 6.883 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 46.693 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "# ÌïôÏäµ ÌõÑ Î©îÎ™®Î¶¨/ÏãúÍ∞Ñ Ï∂úÎ†•\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO4-rHWji7NF"
      },
      "source": [
        "# 3: Applying trained model\n",
        "\n",
        "Apply the trained model to check if it has been successfully fine-tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQrodHfXi4Wq"
      },
      "outputs": [],
      "source": [
        "system_message = 'ÎÑàÎäî ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú Ï£ºÎ¨∏ Î¨∏Ïû•ÏùÑ Î∂ÑÏÑùÌïòÎäî ÏóêÏù¥Ï†ÑÌä∏Ïù¥Îã§. Ï£ºÎ¨∏ÏúºÎ°úÎ∂ÄÌÑ∞ Ïù¥Î•º Íµ¨ÏÑ±ÌïòÎäî ÏùåÏãùÎ™Ö, ÏòµÏÖòÎ™Ö, ÏàòÎüâÏùÑ Ï∞®Î°ÄÎåÄÎ°ú Ï∂îÏ∂úÌï¥Ïïº ÌïúÎã§.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AdQfypJi_PU",
        "outputId": "b57c0ed4-dcba-4ca4-b79d-565904a468cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Î∂ÑÏÑù Í≤∞Í≥º 0: ÏùåÏãùÎ™Ö:ÏßúÏû•Î©¥,ÏàòÎüâ:2Í∑∏Î¶á\n",
            "- Î∂ÑÏÑù Í≤∞Í≥º 1: ÏùåÏãùÎ™Ö:ÏΩúÎùº,ÏàòÎüâ:1Î≥ë<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": \"ÏßúÏû•Î©¥ 2Í∑∏Î¶á, ÏΩúÎùº 1Î≥ë Ï£ºÏÑ∏Ïöî.\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 512, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2-Bx5nmjGpH"
      },
      "source": [
        "# 4: Saving and Loading the Fine-Tuned Model\n",
        "We only save the LoRA adapter (for efficient storage):\n",
        "- Save to the Hugging Face Hub: `push_to_hub`\n",
        "- Save locally: `save_pretrained`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cH4hZnDjEhv",
        "outputId": "d26ae773-c161-4759-a96d-994fce1b404a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/gdrive/MyDrive/Sungshin/2025-2-NLP/fine-tune-tutorial/lora_model/tokenizer_config.json',\n",
              " '/gdrive/MyDrive/Sungshin/2025-2-NLP/fine-tune-tutorial/lora_model/special_tokens_map.json',\n",
              " '/gdrive/MyDrive/Sungshin/2025-2-NLP/fine-tune-tutorial/lora_model/chat_template.jinja',\n",
              " '/gdrive/MyDrive/Sungshin/2025-2-NLP/fine-tune-tutorial/lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(f\"{work_path}/lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(f\"{work_path}/lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi41zViIjNkh"
      },
      "source": [
        "# 5: Loading the saved model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5mCseRUjNL2",
        "outputId": "0753a1e7-16d0-468f-81bf-8d985f4f7ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = f\"{work_path}/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2OkqGH3j8z5",
        "outputId": "37cc2c60-d1dd-4716-df18-abd746734e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Î∂ÑÏÑù Í≤∞Í≥º 0: ÏùåÏãùÎ™Ö:ÏßúÏû•Î©¥, ÏàòÎüâ:2Í∑∏Î¶á\n",
            "- Î∂ÑÏÑù Í≤∞Í≥º 1: ÏùåÏãùÎ™Ö:ÏΩúÎùº, ÏàòÎüâ:1Î≥ë<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 256, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDr0IMVskC5a"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        f\"{work_path}/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"{work_path}/lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fevw_qvFkJSL"
      },
      "source": [
        "# 6: Calculate Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScHiqWQhkKi5"
      },
      "source": [
        "### Q4: Complte the code for calculating BLEU score on the test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "bd41ac93dccb4556a9e452ea916e14da",
            "171b783846d9447892e9a3b443534375",
            "8df4f4a217f04be2a029b9be6c62d9c3",
            "d23dcf766bdf43f89381c56f466b0f4a",
            "66a6b169ed35466caa13fdea75ba1223",
            "3527113fc84544cab84ccefe1fd9af8d",
            "caa68c3a359b4adda017259b822cdd87",
            "c3739dfb8f0f42f59c5001e76f88dd1a",
            "e0fab9cff0004135bd62b5986355f096",
            "2fbe4ec74a7c4b8f9366f610f0b006ac",
            "441ff67dec9842c9992f257b668095a5",
            "cedf7b718fa44344a84f498d2220bc13",
            "29bdb48ca1e045d38e0e5e5183fb9655",
            "5ab006ed304243f5af2a69c63bf33c77",
            "84509c0f92b9482db345ae5daf1e3258",
            "39c535457e284578a7c311ce8b5c6094",
            "d224039c10f041d992390dd933b2f88a",
            "b6696a04c6c7483e950904e17d1a97ac",
            "bbea3c1a05584174946e4a4506b95634",
            "bb1880c2f5cb4c6db1960af8452f9a6b",
            "3c8f436faae94fb09d1296b06b54c901",
            "d180474097c345a59e6c753cfc086f8f",
            "037a02c3d41548409eb158dcd03fc53e",
            "020f01cbf75b45a19e49d0032f38f19f",
            "d6e267bf08c74fe0864f76e35ecaec56",
            "243726cbc4784af7a3e2c32107484af0",
            "bbbad97abf494123b8629255e94bba5e",
            "6759af01ff294c739af60550357900f7",
            "cf564d591b274afd85ee1ec523d12f92",
            "cabee9ac4200431094b5a7928fff29f4",
            "a33bb2ad15c54d818130895caa4dc30e",
            "dc74955b5c11465fb61086c08dbfa776",
            "353431972ef54803b28d575b837feff7"
          ]
        },
        "id": "Ct8Va5eQkG7O",
        "outputId": "e0b40d4f-8909-4a93-fa6d-cb33f8ff935a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd41ac93dccb4556a9e452ea916e14da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cedf7b718fa44344a84f498d2220bc13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "037a02c3d41548409eb158dcd03fc53e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import evaluate\n",
        "bleu_metric = evaluate.load(\"bleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYl5g8IZkNQV",
        "outputId": "ab9df29b-a541-41d8-8f94-f4c0e4444304"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [39:20<00:00,  3.93s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "generated_responses = []\n",
        "reference_responses = []\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Text generation loop\n",
        "for example in tqdm(test_dataset):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": example[\"instruction\"]},\n",
        "        {\"role\": \"user\",   \"content\": example[\"input\"]},\n",
        "    ]\n",
        "\n",
        "    # ÏûÖÎ†• ÌÜ†ÌÅ∞ ÏÉùÏÑ±\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt = True,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Î™®Îç∏Î°ú ÏùëÎãµ ÏÉùÏÑ±\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens = 128,\n",
        "        do_sample = False,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    generated_tokens = outputs[0][input_ids.shape[-1]:]\n",
        "    generated_text = tokenizer.decode(\n",
        "        generated_tokens,\n",
        "        skip_special_tokens = True,\n",
        "    )\n",
        "\n",
        "    reference_text = example[\"output\"]\n",
        "\n",
        "    generated_responses.append(generated_text.strip())\n",
        "    reference_responses.append([reference_text.strip()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpzSsi5LkZtG"
      },
      "outputs": [],
      "source": [
        "result_1 = bleu_metric.compute(\n",
        "    predictions = generated_responses,\n",
        "    references = reference_responses,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsqmZgiDkcs4",
        "outputId": "83f94f47-cb32-47fa-cfe4-57434965457c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8882550232183097\n"
          ]
        }
      ],
      "source": [
        "print(result_1[\"bleu\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23X_FyFMDbu4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1wcKFJfYCFcRCZu1d8oPNE6Ytjs_gaaL6",
      "authorship_tag": "ABX9TyMFTXXOV7M5t/5QFhbGuIIq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}